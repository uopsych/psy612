<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Univariate regression II</title>
    <meta charset="utf-8" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/rladies.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/rladies-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Univariate regression II

---





## Last time...

- Introduction to univariate regression

- Calculation and interpretation of `\(b_0\)` and `\(b_1\)`

- Relationship between `\(X\)`, `\(Y\)`, `\(\hat{Y}\)`, and `\(e\)` 

---

### Today...

Statistical inferences with regression

- Partitioning the variance

- Testing `\(b_{xy}\)`

---

## Statistical Inference

- The way the world is = our model + error

- How good is our model? Does it "fit" the data well? 

--

To assess how well our model fits the data, we simply take all the variability in our outcome and partition it into different categories. For now, we will partition it into two categories: the variability that is predicted by (explained by) our model, and variability that is not.

---

## Partitioning variance (variability)

When it comes to partitioning variance, we often use the sum of the squared deviations for the math.

For one, minimizing the total residual squared deviations is the goal of OLS. 

For another, squared deviations are additive, making the math relatively simple. The total variation of Y is equal to the total residual squared deviations plus the variation of `\(\hat{Y}\)`:

`$$\Large \sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2$$`
???

This is where you draw the **pie chart**
---

`$$\Large \sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2$$`

For simplicity and readability, we may abbreviate these:

`$$\Large SS_Y = SS_{\text{Model}} + SS_{\text{Residual}}$$`

The relative magnitude of sums of squares, especially in more complex designs, provides a way of identifying particularly large and important sources of variability. In the future, we can further partition `\(SS_{\text{Model}}\)` and `\(SS_{\text{Residual}}\)` into smaller pieces, which will help us make more specific inferences and increase statistical power, respectively. 

`$$\Large s^2_Y = s^2_{\hat{Y}} + s^2_{e}$$`
---

## Partitioning variance in Y
- Consider the case with no correlation between X and Y

`$$\Large \hat{Y} = \bar{Y} + r_{xy} \frac{s_{y}}{s_{x}}(X-\bar{X})$$`
`$$\Large \hat{Y} = \bar{Y}$$`

- To the extent that we can generate different predicted values of Y based on the values of the predictors, we are doing well in our prediction

`$$\Large \sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2$$`

`$$\Large SS_Y = SS_{\text{Model}} + SS_{\text{Residual}}$$`

---
## Coefficient of Determination

`$$\Large \frac{s_{Model}^2}{s_{y}^2} = \frac{SS_{Model}}{SS_{Y}} = R^2$$`

`\(R^2\)` represents the proportion of variance in Y that is explained by the model. 

--

`\(\sqrt{R^2} = R\)` is the correlation between the predicted values of Y from the model and the actual values of Y

`$$\large \sqrt{R^2} = r_{Y\hat{Y}}$$`
---



.pull-left[
![](4-regression_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;
]

--

.pull-right[
![](4-regression_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;
]




---
## Example

```r
fit.1 = lm(child ~ parent, data = galton.data)
summary(fit.1) 
```

```
## 
## Call:
## lm(formula = child ~ parent, data = galton.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.8050 -1.3661  0.0487  1.6339  5.9264 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***
## parent       0.64629    0.04114  15.711   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.239 on 926 degrees of freedom
*## Multiple R-squared:  0.2105,	Adjusted R-squared:  0.2096 
## F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16
```

```r
summary(fit.1)$r.squared
```

```
## [1] 0.2104629

NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
*NA```

---
## Example


```r
cor.test(galton.data$parent, galton.data$child)
```

```
## 
## 	Pearson's product-moment correlation
## 
## data:  galton.data$parent and galton.data$child
## t = 15.711, df = 926, p-value &lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.4064067 0.5081153
## sample estimates:
##       cor 
*## 0.4587624
```

--


```r
cor(galton.data$parent, galton.data$child)^2
```

```
## [1] 0.2104629
```

---
## Computing Sum of Squares

`$$\Large \frac{SS_{Model}}{SS_{Y}} = R^2$$`
`$$\Large SS_{Model} = R^2({SS_{Y})}$$`
`$$\Large SS_{residual} = SS_{Y} - R^2({SS_{Y})}$$`

`$$\Large  SS_{residual} = (1- R^2){SS_{Y}}$$`
???
 



![](4-regression_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;

---
## residual standard error


```
## 
## Call:
## lm(formula = child ~ parent, data = galton.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.8050 -1.3661  0.0487  1.6339  5.9264 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***
## parent       0.64629    0.04114  15.711   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
*## Residual standard error: 2.239 on 926 degrees of freedom
## Multiple R-squared:  0.2105,	Adjusted R-squared:  0.2096 
## F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16
```

---
## residual standard error/deviation 

- aka standard deviation of the residual
- aka standard error of the estimate

`$$\Large \hat{\sigma_e}$$`

- interpreted in original units (unlike `\(R^2\)`)
- standard deviation of Y not accounted by model

---

## residual standard error/deviation  



```r
summary(fit.1)$sigma 
```

```
## [1] 2.238547
```

```r
galton.data.1 = broom::augment(fit.1)
psych::describe(galton.data.1$.resid)
```

```
##    vars   n mean   sd median trimmed  mad   min  max range  skew kurtosis   se
## X1    1 928    0 2.24   0.05    0.06 2.26 -7.81 5.93 13.73 -0.24    -0.23 0.07
```

---
## residual standard error/deviation 


```r
summary(fit.1)$sigma 
```

```
## [1] 2.238547
```

```r
psych::describe(galton.data$parent)
```

```
##    vars   n  mean   sd median trimmed  mad min max range  skew kurtosis   se
## X1    1 928 68.31 1.79   68.5   68.32 1.48  64  73     9 -0.04     0.05 0.06
```
---


![](4-regression_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;
---
## `\(R^2\)` and residual standard deviation

- two sides of same coin

- one in original units, the other standardized 

- `\(R^2\)` can be tricky because the numerator and denominator can be changed in different ways. 

- for example if variance in Y is changed but with the same model and residual standard error `\(R^2\)` could decline or increase

---
## Inferential tests

NHST is about making decisions:
  
  - these two means are/are not different
  - this correlation is/is not significant
  - the distribution of this categorical variabe is/is not different between these groups
  
--

In regression, there are several inferential tests being conducted at once. The first is called the **omnibus test** -- this is a test of whether the model fits the data. 

---

### Omnibus test

`$$\Large H_{0}: \rho_{XY}^2= 0$$`

`$$\Large H_{0}: \rho_{XY}^2 \neq 0$$`

It is possible to calculate the significance of your regression with a correlation test. In fact, it would seem quite practical and logical to do so. 

--

However, historically we use a different probability distribution -- **the _F_ distribution** to estimate the signifiance of our model. It's important to know that these methods are mathematically equivalent. But the _F_ distribution is useful here, because it works with our ability to partition variance.

---

![](4-regression_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;


---

## _F _ Distribution review

The F probability distribution represents all possible ratios of two variances:

`$$F \approx \frac{s^2_{1}}{s^2_{2}}$$`
Each variance estimate in the ratio is `\(\chi^2\)` distributed, if the data are normally distributed. The ratio of two `\(\chi^2\)` distributed variables is `\(F\)` distributed. It should be noted that each `\(\chi^2\)` distribution has its own degrees of freedom.

`$$F_{\nu_1\nu_2} = \frac{\frac{\chi^2_{\nu_1}}{\nu_1}}{\frac{\chi^2_{\nu_2}}{\nu_2}}$$`
As a result, _F_ has two degrees of freedom, `\(\nu_1\)` and `\(\nu_2\)`

???

---

## _F_  Distributions and regression

Recall that when using a _z_ or _t_ distribution, we were interested in whether one mean was equal to another mean -- sometimes the second mean was calculated from another sample or hypothesized (i.e., the value of the null). In this comparison, we compared the _difference of two means_ to 0 (or whatever our null hypothesis dictates), and if the difference was not 0, we concluded significance. 

_F_ statistics are not testing the likelihood of differences; they test the size of a _ratio_. In this case, we want to determine whether the variance explained by our model is larger in magnitude than another variance. Which variance?

---

`$$F_{\nu_1\nu_2} = \frac{\frac{\chi^2_{\nu_1}}{\nu_1}}{\frac{\chi^2_{\nu_2}}{\nu_2}}$$`
`$$F_{\nu_1\nu_2} = \frac{\frac{\text{Variance}_{\text{Model}}}{\nu_1}}{\frac{\text{Variance}_{\text{Residual}}}{\nu_2}}$$`

`$$\Large F = \frac{MS_{Model}}{MS_{residual}}$$`
---

.pull-left[
The degrees of freedom for our model are 

`$$DF_1 = k - 1$$`
`$$DF_2 = N-k$$`

Where k is the number of IV's in your model, and N is the sample size. ]

.pull-right[

Mean squares are calculated by taking the relevant Sums of Squares and dividing by their respective degrees of freedom.

- `\(SS_{\text{Model}}\)` is divided by `\(DF_1\)`

- `\(SS_{\text{Residual}}\)` is divided by `\(DF_2\)`
]

---


```r
anova(fit.1)
```

```
## Analysis of Variance Table
## 
## Response: child
##            Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## parent      1 1236.9 1236.93  246.84 &lt; 2.2e-16 ***
## Residuals 926 4640.3    5.01                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

---


```r
summary(fit.1)
```

```
## 
## Call:
## lm(formula = child ~ parent, data = galton.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.8050 -1.3661  0.0487  1.6339  5.9264 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***
## parent       0.64629    0.04114  15.711   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.239 on 926 degrees of freedom
## Multiple R-squared:  0.2105,	Adjusted R-squared:  0.2096 
*## F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16
```


---
## Mean square error (MSE)


- AKA means square residual/within

- unbiased estimate of error variance

    - measure of discrepancy between the data and the model

- the MSE is the variance around the fitted regression line

- Note: this is not the same thing as the residual standard error

---
## regression coefficient

`$$\Large H_{0}: \beta_{1}= 0$$`
`$$\Large H_{1}: \beta_{1} \neq 0$$`
---
## What does the regression coefficient test?
- Does X provide any predictive information? 

- Does X provide any explanatory power regarding the variability of Y? 

- Is the the average value the best guess (i.e., is Y bar equal to the predicted value of Y?)

- Is the regression line flat? 

- Are X and Y correlated?  

---
## Regression coefficient
`$$\Large se_{b} = \frac{s_{Y}}{s_{X}}{\sqrt{\frac {1-r_{xy}^2}{n-2}}}$$`
`$$\Large t(n-2) = \frac{b_{1}}{se_{b}}$$`
** what is standardized equation? 

---
## Intercept

- more complex standard error calculation as the calculation depends on how far the X value (here zero) is away from the mean of X

    - farther from the mean, less information, thus more uncertainty 
    
---

```r
summary(fit.1)
```

```
## 
## Call:
## lm(formula = child ~ parent, data = galton.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.8050 -1.3661  0.0487  1.6339  5.9264 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
*## (Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***
*## parent       0.64629    0.04114  15.711   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.239 on 926 degrees of freedom
## Multiple R-squared:  0.2105,	Adjusted R-squared:  0.2096 
## F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16
```

---

class: inverse

## Next time...

Even more univariate regression!

--

- Confidence amd prediction _bands_

- Model comparison

- Matrix algebra
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
