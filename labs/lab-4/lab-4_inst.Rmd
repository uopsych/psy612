---
title: "Lab 4: Regression with Multiple Continuous Predictors"
output: 
  html_document: 
    fig_caption: yes
    theme: cosmo
    toc: yes
    toc_depth: 3
    toc_float: TRUE
    df_print: paged
---

# Purpose

Today we'll be going over the basics of regression with multiple continuous predictors in R, including reporting the results in tables. 


To quickly navigate to the desired section, click one of the following links:

1. [Partial & Semi-partial Correlations](#semi-partial__partial_correlations)
2. [Regression with Multiple Predictors](#regression_with_multiple_continuous_variables)
3. [Reporting Regressions](#reporting_regressions)

```{r lab-4-1, warning = FALSE, message=FALSE}
# you may need to install:
library(ppcor) # for partial correlations; may need to install
library(janitor) # for cleaning up df names

# you should already have:
library(rio) # for importing data
library(pwr) # for power calculation
library(broom) # for cleaning up output
library(sjPlot) # for tables
library(tidyverse) # for plotting and data wrangling
```

***

# Semi-Partial & Partial Correlations

We'll start by reviewing semi-partial & partial correlations. Recall that zero-order correlations (our old friend) is the strength of the association between two variables, full-stop. Semi-partial & partial correlations index the strength of the association between two variables controlling for a third variable. 

## Semi-partial vs. correlations

Given x1, x2, and Y, the semi-partial correlation between x1 and y (controlling for x2) is the correlation between the part of x1 that is not related to x2 and y:

$$\large r_{Y(X1.X2)} = \frac{r_{YX1}-r_{YX2}r_{X1X2} }{\sqrt{1-r_{X1X2}^2}}$$

Given X1, X2, and Y, the partial correlation between X1 and Y (controlling for X2) is the correlation between the part of X1 that is not related to X2 and the part of Y that is not related to X2:

$$\large r_{YX1.X2}\frac{r_{YX1}-r_{YX2}r_{X1X2} }{\sqrt{1-r_{YX2}^2}\sqrt{1-r_{X1X2}^2}}$$

![](C:/Users/Coryc/OneDrive - University Of Oregon/Work/psy612/lectures/images/venn/Slide5.jpeg)

## Calculating Partial & Semi-partial correlations in R

Let's work on calculating these coefficients in R. To do so, we'll work with that lab 2 dataset we worked with before. Recall that it has:
1. extraversion: self-reported extraversion
2. pos_express: self-reported positive expressivity
3. soc_sup: self-reported social support
4. optimism: self-reported optimism
5. exercise: self-reported exercise
6. happiness: self-reported happiness

```{r}
df <- import("https://raw.githubusercontent.com/uopsych/psy612/master/labs/lab-2/data/PSY612_Lab2_Data.csv") %>% janitor::clean_names() # to get things into snake_case
```

We'll use functions from the {ppcor} library. 

### Semi-partial correlation

To get the semi-partial correlation, we use the `spcor.test()` function. It requires 3 arguments: `x =`, `y =`, and `z =`, which correspond to X1, Y, and X2 above respectively. Put differently, x and y are what you want the correlation between, and z is the variable you want to control for. Let's look at the semi-partial correlation between extraversion and happiness, controlling for social support.

```{r}
ex_happy_socsup_spcor <- spcor.test(
  x = df$extraversion, # X1 (var of interest 1)
  y = df$happiness, # Y (var of interest 2)
  z = df$soc_sup # X2 (control variable)
)

ex_happy_socsup_spcor
```

The resulting object is a dataframe, so we can extract values using `$` notation. For instance, we can get the estimate of the correlation by using `$estimate`

```{r}
ex_happy_socsup_spcor$estimate
```

#### Q1:
>**Question:** What does this value `r round(ex_happy_socsup_spcor$estimate, 2)` mean?

Or the p value by using `$p.value`

```{r}
ex_happy_socsup_spcor$p.value
```

#### Q2:
>**Question:** What does this p value tell us?

#### Semi-partial correlation matrix

We can get the full semi-partial correlation matrix by passing a dataframe to the `spcor()` function, which just has two arguments: `x =` which is where we tell it the df, and `method =` where we could change the method from pearson to another (we'll leave it as its default here).

```{r}
spcor_mat <- spcor(df)
spcor_mat
```

The resulting object is a `list`. It contains three jxj matrices (where j is the number of variables in the df) called:   
1. estimate = a matrix of semi-partial correlations.   
2. p.value = a matrix of p values.
3. statistic = a matrix of *t* statistics.

It also contains 3 additional values:   
1. n = sample size
2. gp = number of control variables (number of variables being 'partialed out')   
3. method = type of correlation   

Like any list, we can extract elements using `$`. So we could get the semi-partial correlation matrix using `$estimate`:

```{r}
spcor_mat$estimate
```

Or get p values with `$p.values`

```{r}
spcor_mat$p.value
```

### Partial correlation

To get the *partial* correlation, we use the `pcor.test()` function. It also requires the same 3 arguments: `x =`, `y =`, and `z =`, which correspond to X1, Y, and X2 above respectively. Put differently, x and y are what you want the correlation between, and z is the variable you want to control for.

```{r}
ex_happy_socsup_pcor <- pcor.test(
  x = df$extraversion, # X1 (var of interest 1)
  y = df$happiness, # Y (var of interest 2)
  z = df$soc_sup # X2 (control variable)
)

ex_happy_socsup_pcor
```

#### Q3:
>**Question:** What does this value `r round(ex_happy_socsup_pcor$estimate, 2)` mean?

#### Q4:
>**Question:** What does is the p value telling us?

#### Partial correlation matrix

Just like with semi-partial correlations, we can get the full partial correlation matrix using the `pcor()` function.

```{r}
pcor_mat <- pcor(df)
pcor_mat
```

# Regression with Multiple Continuous Variables

## Estimating multiple regression in R

One nice thing about regression analysis in R is that it is pretty easy to build up from the simple to the more complex cases. We still use the `lm()` function and the functions we've used over the past few weeks (e.g., `coefdicients()`) to estimate our regression model and extract the information we want. To add multiple predictors, you enter them in the `formula =` argument, separated by a `+`.

Let's run a regression regressing happiness on extraversion and social support:

```{r}
mr_model <- lm(happiness ~ extraversion + soc_sup,
               data = df)
```

And, as we covered a couple of weeks ago, we can get much of the information we want by running `summary()` on it:

```{r}
summary(mr_model)
```

Now we'll go through extracting these pieces of information (separately) and interpreting them. Let's start with coefficients.

### Extracting and interpreting coefficients

We can see the coefficients in the `summary()` object above, but recall from lab 2 that we can also get the coefficients using the `coefficients()` function:

```{r}
coefficients(mr_model)
```

#### Q5:
>**Question:** What does the intercept mean (in plain english, using the estimate in your answer)?

#### Q6:
>**Question:** What does the extraversion term mean (in plain english, using the estimate in your answer)?

#### Q7:
>**Question:** What does the social support term mean (in plain english, using the estimate in your answer)?

We often want NHSTs for each of these parameters, which we can extract the `$coefficients` table from the `summary()` object:

```{r}
summary(mr_model)$coefficients
```

Or, using `broom::tidy()` to get it as a tibble:

```{r}
tidy(mr_model)
```

#### Q8:
>**Question:** Is the intercept signficant? What does that mean?

#### Q9:
>**Question:** Is the extraversion slope significant? What does that mean?

#### Q10:
>**Question:** Is the social support slope significant? What does that mean?

And, we could even get CIs around our estimates:

```{r}
tidy(mr_model, conf.int = TRUE)
```

#### Q11:
>**Question:** What does the CI for the Intercept mean?

#### Q12:
>**Question:** What does the CI for the slope for extraversion mean?

#### Q13:
>**Question:** What does the CI for the  slope for social support mean?

#### Standardized coefficients

Often we want standardized coefficients in addition to unstandardized coefficents. Recall from lecture that the formula for a standardized $\beta$ looks like the following


$$\large \beta_{1} = \frac{r_{YX1}-r_{YX2}r_{X1X2}}{1-r_{X1X2}^2}$$

This would be the formula for the standardized slope for $\beta_1$, or for extraversion in our running example. 

#### Q14:
>**Question:** What do we do to get standardized coefficients in R?

That's right, we z score them first. Let's go ahead and do that now:
```{r}
std_mr_model <- lm(scale(happiness) ~ scale(extraversion) + scale(soc_sup),
               data = df)
```

And we can see the coefficients table using any of the methods above. I'll use tidy here:

```{r}
tidy(std_mr_model)
```
#### Q15:
>**Question:** Why is the intercept zero?

#### Q16:
>**Question:** What does the standardized slope for extraversion mean?

#### Q17:
>**Question:** What does the standardized slope for social support mean?

### Extracting Model Information

Often we want to extract even more information from our model. We can use the same functions we covered in lab 2 to get predicted scores, residuals, a coefficients table, etc. Let's start with predicted y values or fitted values ($\hat{y}$), which we can get using the `fitted.values()` function:

```{r}
fitted.values(mr_model)
```

#### Q18:
>**Question:** Why are there 147 values?

We can get the residual values using `residuals()` function:

```{r}
residuals(mr_model)
```

And we can get our $R^2$ for the model by extracting it from the `summary()` of the model - remember, that isn't stored in the model itself, only in the `summary()` of it.

```{r}
summary(mr_model)$r.squared
```

Which is equal to the squared correlation between the predicted and observed y values:

```{r}
cor(mr_model$fitted.values, df$happiness)^2
```

#### Q19:
>**Question:** What does this value of `r summary(mr_model)$r.squared` mean (in plain english)?

We can instead get the adjusted $R^2$ value that Sara mentioned in lecture yesterday:

```{r}
summary(mr_model)$adj.r.squared
```

#### Q20:
>**Question:** Why is the adjusted $R^2$ `r round(summary(mr_model)$adj.r.squared, 2)` lower than the $R^2$ `r round(summary(mr_model)$r.squared, 2)`?

We can also get our model F by pulling it from the model's `summary()` 
```{r}
summary(mr_model)$fstatistic
```

Which we could lookup the p value using the `pf()` function, providing the f value, numerator df, denominator df (in that order). Don't forget to set `lower.tail = FALSE`:

```{r}
pf(q   = summary(mr_model)$fstatistic[1],
   df1 = summary(mr_model)$fstatistic[2],
   df2 = summary(mr_model)$fstatistic[3],
   lower.tail = FALSE)
```

#### Q21:
>**Question:** Is the Model F significant? What does this mean?

We can see that it is equivalent to testing it against an intercept-only model by doing exactly that. Let's try it. First we fit an intercept only model:

```{r}
int_model <- lm(happiness ~ 1,
                data = df)
```

Then, we can use the `anova()` function to compare them:

```{r}
anova(int_model, mr_model)
```

You can see in the output above that the *F* is exactly what we saw for our model, as is the *p* value.

#### Extracting Model Info with `broom`
And we could of course use `augment()` and `glance()` from {broom} to get tibbles with this information. We can get fitted and residual values (and some other values we'll cover in a few weeks) using augment:

```{r}
augment(mr_model)
```

And we can get those model level statistics using `glance()`:

```{r}
glance(mr_model)
```

# Reporting Regressions

Lastly, we want to make a nice table to report our results. We can use any of the methods we covered in Lab 2. This time, we'll just use `sjPlot::tab_model()`, but `stargazer::stargazer()` and doing it 'by-hand' using broom and `knitr::kable()` are also valid approaches. 

For `tab_model()`, we can just pass it the model, but I'll also give it a title using the `title =` argument and then save it out using the `file =` argument.

```{r}
tab_model(mr_model,
          title = "Regressing Happiness on Extraversion and Social Support",
          file = "tbl/reg_tbl_extra_socsup_happy.doc")
```

# Minihack 1. Get the partial & semi-partial correlation matrices with just social support, positive expressivity, and optimism (no other variables). (Hint: you may need to do some data wrangling.)

## semi-partial
```{r}
df %>% 
  select(soc_sup,
         pos_express,
         optimism) %>% 
  spcor()
```

```{r}
df %>% 
  select(soc_sup,
         pos_express,
         optimism) %>% 
  pcor()
```


# Minihack 2. 

Run a regression predicting happiness from social support, positive expressivity, and extraversion. Then make a new model that has these predictors plus exercise and optimism (5 total predictors). 

```{r}
mr_model_1 <- lm(happiness ~ soc_sup + pos_express + extraversion,
                 data = df)

mr_model_2 <- lm(happiness ~ soc_sup + pos_express + extraversion + exercise + optimism,
                 data = df)
```

What happens to the slopes for social support, positive expressivity, and extraversion after you add optimism and exercise to the model?

```{r}
tidy(mr_model_1)
```

```{r}
tidy(mr_model_2)
```

Next, **formally** compare the overall performance of the models (i.e., get a significance test comparing the models) and examine their adjusted $R^2$ values. Which model is better and how much is it an improvement?

```{r}
anova(mr_model_1,
      mr_model_2)
```

```{r}
summary(mr_model_1)$adj.r.squared
summary(mr_model_2)$adj.r.squared
```

# Minihack 3
Make a table that has both of the models from Minihack 2 **in a single table.**

```{r}
tab_model(mr_model_1, mr_model_2)
```

