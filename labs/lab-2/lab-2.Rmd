---
title: "Lab 2: Univariate Regression"
output: 
  html_document: 
    fig_caption: yes
    theme: cosmo
    toc: yes
    toc_depth: 3
    toc_float: TRUE
---

# Purpose

Today we'll be going over the basics of univariate regression in R, including reporting regression results in Tables. 

To review.

+ Regression is a general approach for data analysis.
+ It can handle a variety of independent variables.
+ It includes everything we might want:
  + Statistical tests (often several).
  + Effect sizes (often several).
  + It can include many independent or non-independent effects.

Today, we'll just be focusing on univariate regression with a single continuous predictor, but over the weeks we will build up into much more complicated regression equations.

To quickly navigate to the desired section, click one of the following links:

1. [Estimating Regression Models](#estimating_regression_models)
2. [NHST in Regression](#nhst_in_regression)
3. [A Tidier Way to Extract Information from Regression Models](#a_tidier_way_to_extract_information_from_regression_models)
4. [Reporting Regressions](Reporting_Regression)

```{r lab-2-1, warning = FALSE, message=FALSE}
library(tidyverse) # for plotting and data wrangling
library(rio) # for importing data
library(psych) # for covariance and correlation functions
library(apaTables) # for correlation tables
library(pwr) # for power calculation
library(broom) # for cleaning up output
library(stargazer)
library(sjPlot)
```

***
# Estimating Regression Models

Even in simple univariate regression, we simultaneously estimate several values at once. Remember, one of our primary goals is estimate $Y$ values with our regression model:

$$Y_i = b_0 + b_1X_i + e_i$$

Alternatively:

$$\hat{Y_i} = b_0 + b_1X_i$$

And sometimes we use greek letters for the model parameters when we're referring to the (generally hypothetical) population parameters:

$$Y_i = \beta_0 + \beta_1X_i + \epsilon_i$$
As Sara mentioned, it is common to use *b* for *unstandardized* slopes and $\beta$ for *standardized slopes* within psychology.

Without further ado, let's go ahead and estimate a regression equation in R.

### Estimating Regressions in R

Conducting regressions in R is actually pretty simple. We use the `lm()` function which is part of the pre-loaded {stats} library. There are basically two ingredients we pass to the `lm()` function

1. **The formula:** Specify your regression formula in the general form `y ~ x`.

2. **The data:** the dataframe that contains the variables in the formula. This is technically optional.

Let's take a look. First we'll import the same data that we used last week:

```{r lab-2-2}
health <- import("https://raw.githubusercontent.com/uopsych/psy612/master/labs/lab-1/data/consc_health.csv")
```

Next we run the model, using `lm()`, setting the formula and data arguments. We want to see to what extent self-reported conscientiousness relates to self-reported health.

```{r lab-2-3}
model <- lm(formula = sr_health ~ consc, # remember, outcome ~ predictors
            data = health) # enter the dataframe
```

More typically, people omit the `formula =`:

```{r lab-2-4}
model <- lm(sr_health ~ consc, # remember, outcome ~ predictors
            data = health)
```

Now our regression model objct, called `model` above, is a `list` object with various useful information. Let's take a look with the `str()` function:

####  {.tabset .tabset-fade .tabset-pills}

##### Code
```{r lab-2-5, eval = FALSE}
str(model)
```

##### Output
```{r lab-2-5b, echo=FALSE, ref.label='lab-2-5'}
```

And we can extract elements from this list like any other list in R, by using `LIST$ELEMENT` or `LIST[["ELEMENT"]]`. There are also specific functions for extracting those elements. Let's start by getting the predicted values of Y.

### Extracting Predicted Y values

We can get predicted values from `model` by subsetting our list with `$`:

#### List Subsetting
#####  {.tabset .tabset-fade .tabset-pills}

###### Code
```{r lab-2-6, eval = FALSE}
model$fitted.values
```

###### Output
```{r lab-2-6b, echo = FALSE, ref.label = "lab-2-6"}
model$fitted.values
```

#### List subsetting II
We can also use double-brackets:

#####  {.tabset .tabset-fade .tabset-pills}

###### Code
```{r lab-2-7, eval = FALSE}
model[["fitted.values"]]
```
###### Output
```{r lab-2-7b, echo = FALSE, ref.label="lab-2-7"}
model[["fitted.values"]]
```

#### `fitted.values()` function
Or the `fitted.values` function:

#####  {.tabset .tabset-fade .tabset-pills}

###### Code
```{r lab-2-8, eval = FALSE}
fitted.values(model)
```
###### Output
```{r lab-2-8b, echo = FALSE, ref.label="lab-2-8"}
fitted.values(model)
```

### Extracting Residuals
We can also extract our residuals or the deviation of each predicted score $\hat{Y_i}$ from the observed scores $Y_i$. That too can be done using list subetting:

#### With List Subsetting

#####  {.tabset .tabset-fade .tabset-pills}

###### Code
```{r lab-2-9, eval = FALSE}
model$residuals
```

###### Output
```{r lab-2-9b, echo = FALSE, ref.label="lab-2-9"}
model$residuals
```

#### With `residuals()` function

Or we could use the `residuals()` function:

#####  {.tabset .tabset-fade .tabset-pills}

###### Code
```{r lab-2-10, eval = FALSE}
residuals(model)
```

###### Output
```{r lab-2-10b, echo = FALSE, ref.label="lab-2-10"}
residuals(model)
```

#### Residuals by hand

And, we could reproduce those values ourselves by subtracting the predicted values we just obtained from the observed Y values from our data 

#####  {.tabset .tabset-fade .tabset-pills}

###### Code
```{r lab-2-11, eval = FALSE}
health$sr_health - fitted.values(model)
```

###### Output
```{r lab-2-11b, echo = FALSE, ref.label="lab-2-11"}
health$sr_health - fitted.values(model)
```

#### Checking our calculations

Let's make sure those are all the same:

#####  {.tabset .tabset-fade .tabset-pills}

###### Code
```{r lab-2-12, eval = FALSE}
round(health$sr_health - fitted.values(model), 8) == round(residuals(model), 8)
```

###### Output
```{r lab-2-12b, echo = FALSE, ref.label="lab-2-12"}
round(health$sr_health - fitted.values(model), 8) == round(residuals(model), 8)
```

#### Sum of Squared Residuals
With this information, we could calculate the sum of squared residuals:

$$\Sigma(Y_i - \hat{Y_i})^2$$

#####  {.tabset .tabset-fade .tabset-pills}

###### Code
```{r lab-2-13, eval = FALSE}
sum((health$sr_health - fitted.values(model))^2)
```

###### Output
```{r lab-2-13b, echo = FALSE, ref.label="lab-2-13"}
sum((health$sr_health - fitted.values(model))^2)
```

#### Standard Error of the Estimate
This of course is one of the building blocks for estimating the **standard error of the estimate**:

$$\sqrt{\frac{\Sigma(Y_i - \hat{Y_i})^2}{N-2}}$$

Let's go ahead and calculate that too:

#####  {.tabset .tabset-fade .tabset-pills}

###### Code
```{r lab-2-14, eval = FALSE}
sqrt(sum((health$sr_health - fitted.values(model))^2)/(nrow(health)-2))
```

###### Output
```{r lab-2-14b, echo = FALSE, ref.label="lab-2-14"}
sqrt(sum((health$sr_health - fitted.values(model))^2)/(nrow(health)-2))
```

### Extracting Standard Error with summary
We can also get the standard error more directly. However, it is not stored in the model list. We have to use the `summary()` function, which gives us some additional useful information about our model. Let's take a look at the structure of it:

#####  {.tabset .tabset-fade .tabset-pills}

###### Code
```{r lab-2-15, eval = FALSE}
str(summary(model))
```

###### Output
```{r lab-2-15b, echo=FALSE, ref.label="lab-2-15"}
str(summary(model))
```

#### Extracting sigma
Remember, R calls the standard error of the estimate `sigma`, so we can get it out of the new summary object using list subsetting:

#####  {.tabset .tabset-fade .tabset-pills}

###### Code
```{r lab-2-16, eval = FALSE}
summary(model)$sigma
```

###### Output
```{r lab-2-16b, echo = FALSE, ref.label="lab-2-16"}
summary(model)$sigma
```

#### Check Equivalency
And we can see that it is equivalent to what we calculated above:

#####  {.tabset .tabset-fade .tabset-pills}

###### Code
```{r lab-2-17, eval = FALSE}
sqrt(sum((health$sr_health - fitted.values(model))^2)/(nrow(health)-2)) == summary(model)$sigma
```

###### Output
```{r lab-2-17b, echo = FALSE, ref.label="lab-2-17"}
sqrt(sum((health$sr_health - fitted.values(model))^2)/(nrow(health)-2)) == summary(model)$sigma
```

#### Q1:
>**Question:** What does a standard error of the estimate of `r round(summary(model)$sigma, 2)` mean? Is that good?


### Extracting the coefficient of determination, $R^2$

Recall that another way we can evaluate our regression model is by extracting the coefficient of determination, $R^2$. This represents the proportion of variance explained by the model. Like the standard error of the estimate, we need to use the summary function to get it:

####  {.tabset .tabset-fade .tabset-pills}

##### Code
```{r lab-2-18, eval = FALSE}
summary(model)$r.squared
```

##### Output
```{r lab-2-18b, echo = FALSE, ref.label="lab-2-18"}
summary(model)$r.squared
```

#### Q2:
>**Question:** What does an $R^2$ of `r round(summary(model)$r.squared, 2)` mean (in plain english)?

### Extracting Regression Coefficients
Recall that we also get estimates for the individual regression coefficients, $b_0$ and $b_1$ in the case of univariate regression. Like many of the other components we've covered, you can extract the coefficients with list subsetting:

#####  {.tabset .tabset-fade .tabset-pills}

###### Code
```{r lab-2-19, eval = FALSE}
model$coefficients
```

###### Output
```{r lab-2-19b, echo = FALSE, ref.label="lab-2-19"}
model$coefficients
```

#### `coefficients()` function

Or we could get the same information with the `coefficients()` function:

#####  {.tabset .tabset-fade .tabset-pills}

###### Code
```{r lab-2-20, eval = FALSE}
coefficients(model)
```

###### Output
```{r lab-2-20b, echo = FALSE, ref.label="lab-2-20"}
coefficients(model)
```
   
#### Q3:
>**Question:** What does the intercept mean?

>**Question:** What about the slope for conscientiousness?


You probably recall that these are called the *unstandardized* coefficients. We can also get the standardized coefficients, but that is a little trickier. Before I show you how to do that:

#### Q4:
>**Question:** How does a standardized coefficient differ from an unstandardized coefficient?


#### Getting standardized coefficients

Standardized regression coefficients, often notated as $\beta$, are just the regression coefficients after the variables have been *standardized* or *Z-scored*. To obtain them, we need to z-score our data with `scale()` before we run the `lm()` function. One really cool thing is that we can do it in the `lm()` call:

####  {.tabset .tabset-fade .tabset-pills}

##### Code
```{r lab-2-21, eval = FALSE}
std_model <- lm(scale(sr_health) ~ scale(consc), data = health)

coefficients(std_model) %>% 
  round(3)
```

##### Output
```{r lab-2-21b, echo = FALSE, ref.label="lab-2-21"}
std_model <- lm(scale(sr_health) ~ scale(consc), data = health)

coefficients(std_model) %>% 
  round(3)
```

#### Q5:
>**Question:** What does the standardized slope for conscientiousness mean?

>**Question:** Why is the intercept zero?

# NHST in Regression

So far, we've been covering how to estimate the various regression components in R and how to extract those components from our model object. However, within the NHST tradition, we also typically want significance tests. With regression, we have several significance tests simultaneously.

## Omnibus test

First, we have the omnibus test, which tests whether or not the regression model significantly outperforms the NULL model. This is typically done with an *F* ratio:

$$F = \frac{MS_{model}}{MS_{residual}}$$
### Getting F
That is also stored in the summary of our model:

####  {.tabset .tabset-fade .tabset-pills}

##### Code
```{r lab-2-22, eval = FALSE}
summary(model)$fstatistic
```

##### Output
```{r lab-2-22b, echo = FALSE, ref.label="lab-2-22"}
summary(model)$fstatistic
```

### Getting p value

And we can use these numbers to look up its p value:

####  {.tabset .tabset-fade .tabset-pills}

##### Code
```{r lab-2-23, eval = FALSE}
pf(summary(model)$fstatistic[1], 
   summary(model)$fstatistic[2], 
   summary(model)$fstatistic[3], 
   lower.tail = FALSE)
```

##### Output
```{r lab-2-23b, echo = FALSE, ref.label="lab-2-23"}
pf(summary(model)$fstatistic[1], 
   summary(model)$fstatistic[2], 
   summary(model)$fstatistic[3], 
   lower.tail = FALSE)
```

### Getting p II

Alternatively, we can pass our model to the `anova()` function, which gives us an F table or ANOVA table for our regression model:

####  {.tabset .tabset-fade .tabset-pills}

##### Code
```{r lab-2-24, eval = FALSE}
anova(model)
```

##### Output
```{r lab-2-24b, echo = FALSE, ref.label="lab-2-24"}
anova(model)
```

#### Q6:
>**Question:** Is the F significant? What does this mean?

## coefficient tests

In addition to the omnibus test, we get a significance test for each of our model's coefficients.

Recall that for each coefficient, we get a *t* test from the formula:

\begin{align}
t = \frac{b_1}{se_b}\\
\\   
\\   
\\   
se_b = \frac{s_Y}{s_X}\sqrt{\frac{1 - r^2_{XY}}{n-2}}
\end{align}

This *t* is distributed with $df = n - 2$.

We can get these from the summary of our model object, by extracting the coefficients *from the summary*.

####  {.tabset .tabset-fade .tabset-pills}

##### Code
```{r lab-2-25, eval = FALSE}
summary(model)$coefficients
```

##### Output
```{r lab-2-25b, echo = FALSE, ref.label="lab-2-25"}
summary(model)$coefficients
```

#### Q7:
>**Question:** Is the test of the intercept significant? What does this mean?

>**Question:** Is the test of the slope significant? What does this mean?

Also, recall that in the case of simple univariate regression, the omnibus F is equivalent to the *t* for the slope squared:

####  {.tabset .tabset-fade .tabset-pills}

##### Code
```{r lab-2-26, eval = FALSE}
summary(model)$fstatistic[[1]]
summary(model)$coefficients[2,3]^2
```

##### Output
```{r lab-2-26b, echo=FALSE, ref.label="lab-2-26"}
summary(model)$fstatistic[[1]]
summary(model)$coefficients[2,3]^2
```

### `Summary()` on its own
Finally, all of this information is displayed in when we just run `summary()`:

```{r lab-2-27}
summary(model)
```

# A Tidier way to Extract Information from Regression Models

You may have noticed at this point that working with lists has its challenges. Even just extracting the information we've extracted so far has some pretty messy code. There must be a better (tidier) way!

Thankfully, there is. This requires the {broom} library, which might be new. It is a package for *tidying* model results objects. It's pretty easy to use - you just pass the model object to a function from {broom} called `tidy`. There are some more advanced things you can do, but just `tidy(model_obj)` (where model_obj is the name of the model) works for most purposes. And, one really nice thing about `broom` is that it works with a lot of different types of models, so this will continue to work as we move to other techniques (e.g., multi-level model with `lme4`).

## broom::tidy()
Let's see what happens when we tidy our model:

```{r lab-2-28}
tidy(model)
```

You can see it produces a dataframe containing the model coefficients and their significance tests.  

## broom::glance()

{broom} also has a function called `glance()` that provides some of the omnibus model information we might want:

```{r lab-2-29}
glance(model)
```

## broom::augment()
`augment` provides more information from our model:

```{r lab-2-30}
augment(model)
```

Using augment, we get the DV, IV, fitted values, residuals, and other diagnostic information we'll learn about in future weeks.

So with broom, just remember:   
1. We `tidy` up coefficients.   
2. We `glance` at (omnibus) model statistics.    
3. We `augment` to find everything else. 

The payoff for these functions comes when you want to do something *with* some of your regression results. As an example, you could use `tidy()` to make it easier to make a plot of your regression coefficients:

```{r lab-2-31}
tidy(model) %>% 
  ggplot(aes(x = term, y = estimate)) +
  geom_point()+
  geom_linerange(aes(ymin = estimate - std.error, 
                 ymax = estimate + std.error))
```

# Reporting Regressions

The last thing we'll cover today is how to report the results of your regression in Tables. We'll cover three different methods, which each have their pro's and con's.

### 'by hand' using broom and kable
Our first option would be to make a table 'by hand' using a combination of `tidy()` and the `kable` function from {knitr}. 
```{r lab-2-32}
tidy(model) %>% # first run tidy on the model.
                # Then pipe it to knitr's kable function
  knitr::kable(digits = c(NA, 2, 2, 2, 3), # set digits; everything rounded to 
                                          # 2 except the labels (NA) and p's (3 digits)
               caption = "Results of Regressing Self-Reported Health on Conscientiousness") # provide a table caption
```

We could clean things up a bit more by changing the names and re-formatting that pesky p value column:

```{r lab-2-33}
tidy(model) %>% # we'll still run tidy on the model first
  # but we'll pass it to rename (part of the tidyverse/dplyr)
  # and rename some of the columns to be more similar to how
  # we normally report these things.
  # rename is pretty easy, it's a way to rename column names
  # the general format is rename(new_name = old_name),
  # where new_name is the new name you want the column to have
  # and old_name is the old name that you're replacing.
  rename(coefficient = term,
        b = estimate,
        SE = std.error,
        t = statistic,
        p = p.value) %>%
  # Then we can mutate the p column to deal with the
  # triple zeroes
  mutate(p = ifelse(p > .001, round(p, 3), "< .001")) %>% 
  knitr::kable(digits = c(NA, 2, 2, 2, 3), # Then we'll do the same as above with kable
               caption = "Results of Regressing Self-Reported Health on Conscientiousness") 
```

This method is nice for two reasons:   
1. You have a lot of control over how things look.   
2. It's pretty general-purpose, and you can easily adapt it to new things you learn how to do in R.   

However, it has a downside in that it is hard to get this into picture perfect APA format (we didn't get all the way there above) and so you may have to do some editing once you get it into word.

### Stargazer

The {stargazer} library can be used to pretty easily get APA formatted tables from many kinds of results or model object. It's super easy to use:

```{r lab-2-34, results = "asis"}
stargazer(model, # give it the model
          type = "html", # tell it the format type.
                        #I'm using html since this is an html doc
          out = "./tbl/reg_tbl_sg.html") # you can give it a path to save
```
   
    
This is nice in that it is super easy, and puts it into APA format for you. It's downsides are that it is less flexible and stargazer may not have a method for your results objct if you're using more advanced or cutting edge tools.

### `sjPlot`
A third option is to use `tab_model()` from the {sjPlot} library. This one does not always work well within the rMarkdown document, but it is **very** easy to export these tables to word. we can do so by setting the file extension of our output to `.doc`:

```{r lab-2-35}
sjPlot::tab_model(model, file = "./tbl/reg_tbl_sjp.doc")
```
    
This has similar cons to stargazer, and additionally doesn't always play nicely with rmarkdown. However, it's super easy to use and even easier than stargazer for exporting tables to MS word.

# Minihack 1

For this minihack, you'll demonstrate the equivalence of a correlation between two variables and the standardized coefficient from a univariate regression regressing one on the other. You'll be working with a dataset called PSY612_Lab2_Data.csv, which is located in the lab-2/data subfolder. It can be downloaded from the following web address:

"https://raw.githubusercontent.com/uopsych/psy612/master/labs/lab-2/data/PSY612_Lab2_Data.csv"

It contains a number of variables, but we'll focus on Extraversion and Happiness first.

Calculate the bi-variate correlation between Extraversion and Happiness. Then, conduct a univariate regression, regressing Happiness on Extraversion and obtain the standardized estimate for the slope. Finally, conduct a series of logical tests showing the equivalence of the estimate (correlation and standardized slope value), their test statistic, and the p value associated with the test statistic (Hint: You can round to 3 digits, which will probably be necessary).

```{r lab-2-36}
# code here
```

# Minihack 2

For this minihack, you'll calculate predicted scores 'by hand' using the regression equation, compare them to the predicted scores stored in the model, and finally use the predicted scores you calculate to estimate $R^2$. We'll work with the same dataset, but this time you'll regress social support (SocSup) on Extraversion.

a.) Run the regression model. Extract the coefficients, calculate the predicted scores **using the extracted coefficients** (HINT: think about the regression equation).

```{r lab-2-37}
# code here
```


b.) Demonstrate that the predicted scores are the same as the values from `fitted.values`.

```{r lab-2-38}
# code here
```

c.) Use those predicted scores to calculate $R^2$. Demonstrate its equivalence to the result of `summary(model)$r.squared`.

```{r lab-2-39}
# code here
```

# Minihack 3

Create two tables using two different methods we covered today. The first table should correspond to the regression from Minihack 1 (regressing happiness on Exraversion) and the second should correspond to the regression from Minihack 2 (regressing social support on Extraversion).

```{r lab-2-40}
# Code here
```

```{r lab-2-41}
# code here
```
