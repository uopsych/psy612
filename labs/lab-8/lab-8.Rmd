---
title: "Lab 8: Factorial ANOVA"
output: 
  html_document: 
    fig_caption: yes
    theme: cosmo
    toc: yes
    toc_depth: 3
    toc_float: TRUE
    df_print: paged
editor_options: 
  chunk_output_type: console
---

You will need the Rmd file for today's lab - can download it [here](https://uopsych.github.io/psy612/labs/lab-8/lab-8.Rmd).


Be sure to have the following packages installed and loaded:

```{r lab-8-1, message=FALSE}
library(tidyverse) # for plotting and data wrangling
library(rio) # for importing data
library(psych) # for descriptives
library(lsr) # for eta squared functions
library(emmeans) # for marginal means and simple effects
library(sjPlot) # for plotting model results 
library(apaTables) # for tables of means
library(car) # for testing model assumptions
library(broom) # for tidying model output

# suppress scientific notation
options(scipen = 999)
```


# Purpose

Factorial ANOVA (aka multiple regression with categorical predictors) refers to a special case of the general linear model in which there is an interaction of two or more categorical variables (i.e. *factors*). A factorial design is used when there is an interest in how two or more variables (or factors) affect some outcomes variable. Rather than conduct separate one-way ANOVAs for each factor, they are all included in one analysis. Today we will review how to run factorial ANOVA models in R and how to interpret and visualize the results. 


# Research scenario

Based on subjects' self-reports of rejection sensitivity (N = 80), a researcher divides subjects into two equal groups (low RS and high RS).  Whereas half of the subjects in each group interact with a partner who displays a happy emotional expression during the interaction, the other half of the subjects in each group interact with a partner who displays a neutral emotional expression during the interaction.  After the interaction, subjects are asked to rate the statement, "My interaction partner likes me", on a scale from 1 (strongly disagree) to 7 (strongly agree).

Factor 1: Rejection Sensitivity
- Low
- High

Factor 2: Partner's Emotional Expression
- Neutral
- Happy

Dependent Variable: Perceived Liking



# Hypothesis Testing

With a factorial ANOVA, three different null hypotheses can be tested regarding whether there is 1) a main effect of factor 1, 2) a main effect of factor 2, and 3) an interaction between factor 1 and factor 2.

Imagine a table with means like the following:

|         |High | Low   | Mean  |
| ------- |-----| ----- | ----- |
| Neutral |10   | 10    | 10    |
| Happy   |10   | 10    | 10    |
| Mean    |10   | 10    | 10    |

**Null Hypothesis 1** (Main Effect of Rejection Sensitivity):
$$H_0: \mu_{Low} = \mu_{High}$$
Another way to state this hypothesis:
$$\alpha_{Low} = \mu_{Low} - \mu_{all}$$
$$\alpha_{High} = \mu_{High} - \mu_{all}$$
$$H_0: \alpha_{Low} = \alpha_{High} = 0$$

**Null Hypothesis 2** (Main Effect of Partner's Emotional Expression):
$$H_0: \mu_{Neutral} = \mu_{Happy}$$
Again, another way to state this hypothesis:
$$\beta_{Neutral} = \mu_{Neutral} - \mu_{all}$$
$$\beta_{Happy} = \mu_{Happy} - \mu_{all}$$
$$H_0: \beta_{Neutral} = \beta_{Happy} = 0$$

**Null Hypothesis 3** (Interaction Effect):

H_0: There is no interaction between rejection sensitivity and partner's emotional expression.

Or, lastly, in another way:

$$\alpha\beta_{Low+Neutral} = \mu_{Low+Neutral} - \alpha_{Low} - \beta_{Neutral} - \mu_{all}$$
$$\alpha\beta_{High+Neutral} = \mu_{High+Neutral} - \alpha_{High} - \beta_{Neutral} - \mu_{all}$$
$$\alpha\beta_{Low+Happy} = \mu_{Low+Happy} - \alpha_{Low} - \beta_{Happy} - \mu_{all}$$
$$\alpha\beta_{High+Happy} = \mu_{High+Happy} - \alpha_{High} - \beta_{Happy} - \mu_{all}$$
$$H_0: \alpha\beta_{Low+Neutral} = \alpha\beta_{High+Neutral} = \alpha\beta_{Low+Happy} = \alpha\beta_{High+Happy} = 0$$


# Import & inspect the Data

```{r lab-8-2}
reject <- import("https://raw.githubusercontent.com/uopsych/psy612/master/labs/lab-8/data/reject.csv")
```

```{r}
str(reject)
head(reject)
```

* It looks like `rs` and `partner` are both being read in as character variables. Let's go ahead and change those to factors.

```{r lab-8-7}
reject <- reject %>% 
  mutate(rs = as.factor(rs),
         partner = as.factor(partner))
```

* Check the structure again. 

```{r}
str(reject)
```


Notice that by default R orders factor levels alphabetically. In our case, this means that `High` will be the reference group of rejection sensitivity and `Happy` will be the reference group of interaction partner's emotional expression. However, it might be more intuitive to have `Low` and `Neutral` be the reference groups, respectively. 


* To accomplish this, we can simply re-order the levels of our factor variables with `fct_relevel()`.
```{r}
reject <- reject %>% 
  mutate(rs = fct_relevel(rs, c("Low", "High")), 
         partner = fct_relevel(partner, c("Neutral", "Happy")))

str(reject) # Notice the re-ordering of levels
```


* We can also see how the factors are being coded using contrasts().
```{r}
contrasts(reject$rs)
contrasts(reject$partner)
```

Notice that each are being dummy coded. It's very important to be aware of how the factors in your regression model are being coded to be able to accurately interpret the regression coefficients in your output.

Speaking of regression, note we are going to conduct the analysis using the lm() function. The following is how you could write the factorial ANOVA as the regression equation that we're estimating:

$$Liking_i = \beta_0 + \beta_1Partner_i + \beta_2Liking_i + \beta_3PartnerxLiking_i + \epsilon_i$$


# Descriptive Statistics

Obtaining sample size per condition.

```{r}
reject %>%
  group_by(rs, partner) %>% 
  summarize(n = n())
```


## Table of Means

The results of a factorial ANOVA are often represented in a table of means, which contains the means of each combination of conditions (the cell means) and means for the levels of each variable ignoring the other variables (the marginal means).

The following can be used to obtain cell means.
```{r}
reject %>%
  group_by(rs, partner) %>% # Group by both rs and partner
  summarize(mean = mean(liking, na.rm = TRUE), # Obtain the mean for each condition
            sd = sd(liking, na.rm = TRUE)) %>%  # Obtain the SD for each condition
knitr::kable(digits = c(NA, NA, 2, 2, 2), # Specifying number of digits to report
             caption = "Cell Means & SD")
```


The following can be used to obtain marginal means for rejection sensitivity.
```{r}
# Marginal Means for Rejection Sensitivity
reject %>%
  group_by(rs) %>% # Just group by rejection sensitivity
  summarize(mean = mean(liking, na.rm = TRUE),
            sd = sd(liking, na.rm = TRUE)) %>% 
knitr::kable(digits = c(NA, 2, 2, 2),
             caption = "Marginal Means & SD for Rejection Sensitivity")
```

And the marginal means for partner's emotional expression.
```{r}
# Marginal Means for Partner's Emotional Expression
reject %>%
  group_by(partner) %>% # Just group by partner
  summarize(mean = mean(liking, na.rm = TRUE),
            sd = sd(liking, na.rm = TRUE)) %>% 
knitr::kable(digits = c(NA, 2, 2, 2),
             caption = "Marginal Means & SD for Partner's Emotional Expression")
```


The following can be used to obtain the grand mean (the overall mean on the DV).
```{r}
reject %>% # note that we don't need group_by for this one
  summarize(mean = mean(liking, na.rm = TRUE),
            sd = sd(liking, na.rm = TRUE)) %>% 
knitr::kable(digits = c(2, 2, 2),
             caption = "Grand Mean")
```


## Table of Means - An Easier Way

* The apa.2way.table() function from the apaTables package is a much more convenient way to get our cell means and marginal means. This function works for any type of 2-way ANOVA, regardless of the number of levels of your factors, e.g. it would work for a 3 x 3 ANOVA. All you need to do is indicate what the IV's (aka factors) and DV are and specify `show.marginal.means = TRUE`. 

```{r}
table_of_means <- apa.2way.table(iv1 = rs, 
               iv2 = partner, 
               dv = liking, 
               data = reject,
               show.marginal.means = TRUE)
table_of_means
```


We can also get the output of the apa.2way.table() function exported to a word document.
```{r}
apa.2way.table(iv1 = rs,
               iv2 = partner,
               dv = liking,
               data = reject,
               show.marginal.means = TRUE,
               filename = "table_of_means.doc")
```

>**Question:** Which means are being compared in the main effect of rejection sensitivity?

>**Question:** Which means are being compared in the main effect of interaction partner?

>**Question:** Which means are involved in the interaction?

# Conduct the Factorial ANOVA 

Factorial ANOVA is the method by which we can examine whether two (or more) categorical IVs have joint effects on a continuous outcome of interest. Like all general linear models, factorial ANOVA is a specific case of multiple regression. However, we may choose to use an ANOVA framework for the sake of interpretability.

```{r}
model <- lm (liking ~ rs*partner, data = reject)
```

## Results in regression framework

```{r}

```

>**Question:** What do each of the regression coefficient estimates mean? 

b0 = 5.55

b1 = -3.75

b2 = 0.45

b3 = 4.20

It may be helpful to refer back to the table of means.
```{r}
table_of_means
```



## Getting Factorial ANOVA Results using anova()

Obviously, the means being compared by b1 and b2 do not represent our main effects. The main effect of RS would be a comparison of the marginal means for the low and high conditions. The main effect of partner would be a comparison of the marginal means for the neutral and happy conditions.

With the way we have rs and partner coded in the model, the regression coefficient estimates don't correspond simply to the mean difference between conditions. For testing our main effects, you can get more straightforward results using anova().

```{r}
anova(model)
```


# Calculating the SS for each term

Recall the null hypothesis being tested by each row in the ANOVA table.

### Null Hypothesis 1 (Main Effect of Rejection Sensitivity):
$$H_0: \mu_{Low} = \mu_{High}$$
$$\alpha_{Low} = \mu_{Low} - \mu_{all}$$
$$\alpha_{High} = \mu_{High} - \mu_{all}$$
$$H_0: \alpha_{Low} = \alpha_{High} = 0$$

>**Question:** Where do we look for variability to examine the effect of rejection sensitivity on liking? 



Recall from lecture the formula for calculating SS for the effect of the row variable is:

![](https://raw.githubusercontent.com/saralieber/R_Coding/master/SS_Row.PNG)

Let's calculate it from scratch. 
```{r}
# Calculating the grand mean
GM <- mean(reject$liking, na.rm = TRUE) 
GM

# Marginal Means for Rejection Sensitivity
reject %>%
  group_by(rs) %>% 
  summarize(mean = mean(liking, na.rm = TRUE)) %>% 
knitr::kable(digits = c(NA, 5))


# Recall number of people per row and column
reject %>%
  group_by(rs, partner) %>% 
  summarize(n = n())


# Calculate SS_rs
SS_rs <- 2*20*((4.125-GM)^2+ (5.775-GM)^2)
SS_rs


# Compare to anova() output
anova(model)
```


### Null Hypothesis 2 (Main Effect of Partner's Emotional Expression):
$$H_0: \mu_{Neutral} = \mu_{Happy}$$
Or,
$$\beta_{Neutral} = \mu_{Neutral} - \mu_{all}$$
$$\beta_{Happy} = \mu_{Happy} - \mu_{all}$$
$$H_0: \beta_{Neutral} = \beta_{Happy} = 0$$

>**Question:** Where do we look for variability to examine the effect of partner's emotional expression on liking? 

The formula for calculating SS for the effect of the column variable is:

![](https://raw.githubusercontent.com/saralieber/R_Coding/master/SS_Column.PNG)

Let's calculate it from scratch.
```{r}
# Marginal Means for Partner's Emotional Expression
reject %>%
  group_by(partner) %>% 
  summarize(mean = mean(liking, na.rm = TRUE)) %>% 
knitr::kable(digits = c(NA, 5))


# Calculate SS_partner
SS_partner <- 2*20*((6.225-GM)^2 + (3.675-GM)^2)
SS_partner


# Compare to anova() output
anova(model)
```

### Null Hypothesis 3 (Interaction Effect):

H0: There is no interaction between rejection sensitivity and partner's emotional expression.

$$\alpha\beta_{Low+Neutral} = \mu_{Low+Neutral} - \alpha_{Low} - \beta_{Neutral} - \mu_{all}$$
$$\alpha\beta_{High+Neutral} = \mu_{High+Neutral} - \alpha_{High} - \beta_{Neutral} - \mu_{all}$$
$$\alpha\beta_{Low+Happy} = \mu_{Low+Happy} - \alpha_{Low} - \beta_{Happy} - \mu_{all}$$
$$\alpha\beta_{High+Happy} = \mu_{High+Happy} - \alpha_{High} - \beta_{Happy} - \mu_{all}$$
$$H_0: \alpha\beta_{Low+Neutral} = \alpha\beta_{High+Neutral} = \alpha\beta_{Low+Happy} = \alpha\beta_{High+Happy} = 0$$

>**Question:** Where do we look for variability to examine whether there is an interaction between rejection sensitivity and partner's emotional expression?


The formula for calculating SS for the effect of the interaction effect is:

![](https://raw.githubusercontent.com/saralieber/R_Coding/master/SS_Interaction.PNG)


This is a simplification of doing the following:

![](https://raw.githubusercontent.com/saralieber/R_Coding/master/SS_Int_full.PNG)


In other words, is there any variability remaining among the cell means that is not completely explained by the main effects of the row and column variable?


Go ahead and try calculating the SS for the interaction from scratch using the formula above. 
```{r}

# Cell Means
reject %>%
  group_by(rs, partner) %>%
  summarize(mean = mean(liking, na.rm = TRUE)) %>% 
knitr::kable(digits = c(NA, NA, 5))

table_of_means

# Calculate SS_interaction
SS_Int <- 20*((5.55-5.78-3.67+GM)^2+(1.8-4.12-3.67+GM)^2+(6-5.78-6.22+GM)^2+(6.45-4.12-6.22+GM)^2)

# Compare to anova() output
anova(model)
```



# Calcuating MS

The variance, or MS, for each term is simply calculated by taking the SS for that term and dividing it by the degrees of freedom for that term.

Recall the formulas for calcualting df:

- df_Row = R - 1
- df_Column = C - 1
- df_Interaction = (R-1)(C-1)

where R is the number of rows and C is the number of columns in your table of means.

```{r}
MS_rs <- 54.45/1 
MS_partner <- 130.05/1
MS_Interaction <- 88.2/1
```


# Calucating the F-statistic for each test

To finally test each null hypothesis, we need to calculate an F-statistic and either compare that F-statistic to a critical value or examine the probability of obtaining an F-statistic as extreme as the one we obtained if the null hypothesis were true (aka, the p-value).

>**Question:** The MS for each term that we calculated above will be the numerator of our F-statistic. What do we use as the denominator?

```{r}

F_rs = -99
F_partner = -99
F_Interaction = -99

F_rs
F_partner
F_Interaction

anova(model)

```


>**Question:** What is our conclusion about each of our null hypotheses?

# Effect Sizes (Eta-Squared and Partial Eta-Squared)

Effect size is really important to report. It provides an idea of the *size* of the effect of a predictor variable on an outcome variable. Reporting effect size in publications is also very helpful for other researchers wanting to conduct a priori power analyses.

```{r}
etaSquared(model)
```

>**Question:** What is the difference between eta-squared and partial eta-squared? Do you think one has strengths over the other? 

>**Question:** Sara will go over that on the coming Tuesday's lecture!

# Visualization
Visualizing the results is helpful for understanding your results and interpreting significant (or non-significant) effects. Especially when there is a significant interaction effect, a plot can help illustrate what the interaction effect is.  

### Main effects

* Remember that main effects correspond to differences in marginal means. To plot main effects, we can use the plot_model() function from the sjPlot package. This function takes three arguments: 1) the fitted model object, 2) because we want to plot marginal means, we specify `type = emm`, and 3) Specify in the `terms = ` argument which variable(s) you want the marginal means for.

### Plotting the Main Effect of Rejection Sensitivity
```{r}
plot_model(model, type = "emm", terms = "rs")
```

### Plotting the Main Effect of Partner's Emotional Expression
```{r}
plot_model(model, type = "emm", terms = "partner")
```

### Plotting the Interaction Effect
```{r}
plot_model(model, type = "emm", terms = c("rs", "partner")) 
```

Switch how the interaction is visualized by switching the order of `terms`.

>**Question:** How would you describe in a paper what the significant interaction effect means? 

# Simple effects

Simple effects are the effect of some factor (e.g., interaction partner's expression) at each level of another factor (e.g., at high and low RS separately).

* We'll look at the simple effect of interaction partner having a neutral vs. happy expression on perceived liking at different levels of rejection sensitivity. We'll use the `{emmeans}` package.

* To get simple effects, we combine the `emmeans()` function with the `contrast()` function (both from `{emmeans}`). `emmeans()` works by passing it a model and then specifying which variables you're looking at. Then, we pass that along to `contrast()`, which can give us a variety of different contrasts. 

* If we want simple effects for interaction partner expression at each level of rejection sensitivity we can use `by = "rs"` and `simple = "partner"`.

```{r}
model %>% emmeans(specs = c("partner", "rs")) %>% # specify our two factors
  contrast(by = "rs", # by is the variable we are looking at each level of
           simple = "partner") # simple is what we want simple effects of
```

>**Question:** What do the results of this simple effects analysis show? 

* Now let's look at simple effects for rejection sensitivity at each level of partner expression.

```{r}
model %>% emmeans(specs = c("partner", "rs")) %>% # specify our two factors
  contrast(by = "partner", # by is the variable we are looking at each level of
           simple = "rs") # simple is what we want simple effects of
```

>**Question:** What do the results of this simple effects analysis show? 