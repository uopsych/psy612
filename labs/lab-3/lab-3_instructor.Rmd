---
title: "Lab 3: Univariate Regression (II) & GLM"
output: 
  html_document: 
    fig_caption: yes
    theme: cosmo
    toc: yes
    toc_depth: 3
    toc_float: TRUE
    df_print: paged
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, rows.print = 10)

# suppress scientific notation
options(scipen = 999)
```

You can download the Rmd file [here](lab-3.Rmd) to follow along.

# Purpose

Today we will review univariate regression, discuss how to summarize and visualize uncertainty in regression models, and discuss how to estimate regression coefficients using matrix algebra. At the end, we will introduce the General Linear Model and demonstrate how GLM can be used to understand all of the statistical tests we have learned so far (*t*-tests, ANOVA, correlations, regressions) within one unifying framework. 

For today's lab, you will need to load the following libraries:

```{r, message = FALSE, warning = FALSE}
library(tidyverse) # for plotting and data wrangling
library(rio) # for importing data
library(broom) # for cleaning
library(sjPlot) # for plotting
library(ggpubr) # for plotting
library(carData) # for Guyer dataset
```

***

# Visualizing uncertainty

For today's lab, we are going to continue using the same dataset from previous labs that contains variables measuring gender, self-reported conscientiousness (from the BFI), and self-reported physical health.

## Data and review

First, let's load in the data.

```{r}
health <- import("https://raw.githubusercontent.com/uopsych/psy612/master/labs/lab-3/data/consc_health.csv")
```


Our model can be described with the following regression equation:

$$health_i = b_0 + b_1consc_i + e_i$$

We can run the model in R using the `lm()` function.

```{r}
model <- lm(sr_health ~ consc, data = health) # regression of y on x
summary(model)
```

> **Question:** What do the intercept and slope mean? What do the *p*-values tell us? 




*pause and let students answer*












The meaning of regression coefficient test:
- Is coefficient = 0?
- does X provide any predictive info?
- Does X provide any explanatory power regarding the variability of Y?
- Is the average value different from the best guess (i.e., is Y_bar equal to Y_hat)?
- Is the regression line not flat?
- Are X and Y correlated?

Slope CI: Based on standard error for the slope coefficient, it represents the uncertainty (noise) in our estimate of the regression coefficient. Different from (but proportional to) the standard error of the estimate. We can take our estimate and put confidence regions around it to get an estimate of what could be "possible" if we ran the study again. CI_b = b +/- CV(se_b)

omnibus test null hypothesis:
$$H_0: \rho^2_{Y\hat{Y}} = 0$$
(where Y_hat is the predictive value from the equation.)


## Confidence intervals

Our `b's` (intercept and slope) are *estimates* from our sample of true population parameters ($\beta$'s). Whenever we calculate an estimate, we should also determine how precise our estimate is. 

Recall the formula for calculating confidence intervals:

$$CI_b = b \pm CV(SE_b)$$

We can calculate the confidence interval around the estimates in R with the function `stats::confint()`. This function takes the model object as the first argument. By default it will give you 95% CI's. 

```{r}
confint(model)
```


>**Question:** What does these 95% CI for the slope of conscientiousness mean in plain English? 



*pause and let students answer*














If we ran the study for a lot of times, 95% of the times the slope will land between these two numbers. 

If I were to repeat this study, an infinite number of times 95% of the time this confidence interval would capture the true population parameter the truth slope estimate overpopulation I can also get a 95% confidence interval around my intercept the same way.

## Confidence bands

In addition to estimating precision around the our coefficients, we can also estimate our precision around each fitted value, $\hat{Y_i}$. These standard errors are generated by `broom::augment()` (and are labeled `.se.fit`).

```{r}
model %>% 
  augment(se_fit = TRUE) %>%
  select(sr_health, .fitted, .se.fit) %>% 
  head() 
```

If we were to string all of this information together, it would generate a confidence **band** around our regression line. As we've already seen, you can get this confidence band when creating a scatter plot by adding `geom_smooth(method = "lm")`. 

```{r, message = FALSE}
health %>%
  ggplot(aes(x = consc, y = sr_health)) + 
  geom_point() + 
  geom_smooth(method = "lm") + # adds a layer that includes the regression line & 95% confidence band
  labs(x = "Conscientiousness", y = "Self-rated health") +
  theme_minimal()
```

Thinner at mean_x, mean_y, and expanding wider at more extreme values.

The animation below is an example of a ["Hypothetical Outcomes Plot"](https://github.com/wilkelab/ungeviz){target="_blank"} (HOP) that visually demonstrates what this 95% CI band represents. In essence, this plot shows what the regression line could look like if we were to repeat our experiment over and over (sampling from the same population each time). 

```{r echo=FALSE, warning = FALSE, message = FALSE}
#remotes::install_github("wilkelab/ungeviz")
library(ungeviz)
library(gganimate)
library(transformr)
library(gifski)

set.seed(1)

boots <- bootstrapper(100)

p <- health %>%
  ggplot(aes(x = consc, y = sr_health)) +
  geom_smooth(method = "lm", color = NA) +
  geom_point(alpha = 0.3) +
  geom_smooth(data = boots, method = "lm", fullrange = TRUE, se = FALSE) +
  theme_minimal() +
  labs(x = "Conscientiousness", y = "Self-rated health") +
  transition_states(.draw, 1, 1) +
  enter_fade() +
  exit_fade() +
  ease_aes()

animate(p, fps = 3)

```


## Prediction bands

The `predict()` function allows us to get the fitted `Y` values from all 60 conscientiousness values in our dataset.  Note: you can also get the fitted `Y` values with the `broom::augment()` function like we did earlier in the lab.


```{r}
predict(model)

model %>% 
  augment(se_fit = TRUE) %>%
  select(.fitted) %>% 
  head() 
```

We can use this information to create "prediction bands". First we will generate our fitted values along with a "prediction interval" (lower and upper bound) for each of these values. 

```{r, warning = FALSE}
fitted_interval <- predict(model, interval = "prediction")
head(fitted_interval)
```

Next we'll bind these fitted values (and their intervals) to our original dataset. 

```{r}
new_df <- cbind(health, fitted_interval)
head(new_df)
```

And finally, we'll plot a prediction band on top of the data by adding a `geom_ribbon()` layer.

```{r, message = FALSE}
new_df %>% 
  ggplot(aes(x = consc, y = sr_health)) +
  geom_point() +
  geom_smooth(method = lm, se = TRUE) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), fill = "blue", alpha = 0.1) + 
  labs(x = "Conscientiousness", y = "Self-rated health") +
  theme_minimal()
```

>**Question:** What does the prediction band represent? How does it differ from the confidence band? 



*pause and let students answer*













Equation for standard error around predicted values: https://uopsych.github.io/psy612/lectures/05-regression.html#/26. Each person gets their own se. (confidence band)

Confidence band: Precision of estimate of y. how extreme (i.e flat/steep) the line could be. 95% of the confidence interval. 

Confidence band index the precision of our best guess (estimate of mean). 

Prediction band: https://uopsych.github.io/psy612/lectures/5-regression.html#32. Index the likely value of y for a given x. Combining unknown variability of estimated mean with standard error of the estimate. How much do you expect individual to vary around our best guess. Precison of scores. 



# The General Linear Model

We just saw that regression works "under the hood" by solving a matrix algebra equation to get the intercept and slope of the regression line. As Sara briefly mentioned in class, this matrix algebra equation works for *any* type of data we have. This should clue us into the idea that there is some fundamental process going on behind the scenes of all of our models...

The **general linear model (GLM)** is a family of models that assume the relationship between your DV and IV(s) is additive, and that your outcome is normally distributed. In its simplest form, we can think of the general linear model as: 

$$Data = Model + Error $$ 

This provides a unifying framework for all of the statistical tests we have learned (or at least touched on) so far: *t*-tests, correlations, ANOVA, and linear regression. All of these tests are really doing the same math at the end of the day.

To illustrate this, let's look at the relationship between gender and self-reported health from the `health` data set.


```{r}
head(health)
str(health)
```

Let's covert the `gender` variable to `0's` and `1's`. We will talk more about dummy coding in a future lab. 


```{r}
health <-  health %>% 
  mutate(gender = case_when(gender == "male" ~ 0,
                               gender == "female" ~ 1))
head(health)
```

## *t*-test

```{r}
t_test <- t.test(formula = sr_health ~ gender, data = health, var.equal = TRUE)
t_test
```

## Correlation 

```{r}
cor_test <- cor.test(formula = ~ sr_health + gender, data = health)
cor_test
```

## ANOVA

```{r}
anova_test <- aov(formula = sr_health ~ gender, data = health)
summary(anova_test)

```

## Regression

```{r}
regression <- lm(formula = sr_health ~ gender, data = health)
summary(regression)
```

## Comparison

```{r}
t_test$p.value
cor_test$p.value
anova(anova_test)$`Pr(>F)`[1]
summary(regression)$coefficients[2,4]
```

***

# Minihacks

## Minihack 1: Prediction and confidence bands

Using the `mtcars` dataset, create a scatterplot of cyl (x-axis) and mpg (y-axis) that includes both a prediction band and a confidence band.

1. Run a regression model.

```{r}
model <- lm(mpg ~ cyl, data = mtcars)
summary(model)
```

2. Generate fitted values and their prediction interval using the `predict()` function.
```{r}
fitted_interval <- predict(model, interval = "prediction")
head(fitted_interval)

```

3. Bind the fitted values and the prediction intervals to the original dataset. 

```{r}
mtcars_n <- cbind(mtcars, fitted_interval)
head(mtcars_n)

```

4. Using ggplot(), create a scatterplot that includes both a prediction band and a confidence band.

```{r}
mtcars_n %>% 
  ggplot(aes(x = cyl, y = mpg)) +
  geom_point() +
  geom_smooth(method = lm, se = TRUE) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), fill = "blue", alpha = 0.1) + 
  labs(x = "cyl", y = "mpg") +
  theme_minimal()
```

***

## Minihack 2: Confidence intervals

For this minihack, we will refer back to the example about conscientiousness and health. We used `confint()` to calculate the 95% CI for our regression coefficients. Your job is start with the model output, stored as a list object (see below), and extract the relevant pieces of information to calculate the 95% CI around the intercept and slope.

Note: the formula for a confidence interval is estimate +- cv*se

```{r}
model <- lm(sr_health ~ consc, data = health)
summary_model <- summary(model)
```

1. Extract the estimates from the model object.

```{r}
int <- summary_model$coefficients[[1,1]]
slope <- summary_model$coefficients[[2,1]]
```

2. Extract standard errors from the model object. Hint: use summary_model$coefficients.

```{r}
int_se <- summary_model$coefficients[[1,2]]
slope_se <- summary_model$coefficients[[2,2]]
```

3. Extract the df (It will be the denominator df from F statistic)).

```{r}
df <- summary_model$fstatistic[["dendf"]]

```

4. Calculate the critical value.
```{r}
cv <- qt(.975, df = df)
```

5. Calculate a 95% CI for both estimates.

```{r}
# intercept 95% CI 
int_ci_l <- int - cv*int_se  
int_ci_u <- int + cv*int_se 
paste0("[", round(int_ci_l,2), ",", round(int_ci_u,2), "]")

# slope 95% CI 
slope_ci_l <- slope - cv*slope_se
slope_ci_u <- slope + cv*slope_se
paste0("[", round(slope_ci_l,2), ",", round(slope_ci_u,2), "]")

```


6. Verify that your answer corresponds to the result from `confint()`. 

```{r}
ci <- confint(model)
ci
```
